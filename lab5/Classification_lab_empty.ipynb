{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification lab no solution",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CD5FUmV-JlU",
        "colab_type": "text"
      },
      "source": [
        "# Classification models \n",
        "CDS Class, prepared by Prof. Dorien Herremans, Cheuk Kin Wai, and Yin-Jyun Luo. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Let's start by loading the necessary libraries for this class. The main libraries used are numpy and pandas for data handling and [scikit learn (sklearn)](https://scikit-learn.org/stable/) for classification models. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRVVQV0NQk6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyJ9gKmxQz4G",
        "colab_type": "text"
      },
      "source": [
        "## Walkthrough example of different classification methods\n",
        "\n",
        "To become familiar with the classification models, please follow the instructions below. \n",
        "\n",
        "### Problem description and preparing dataset\n",
        "\n",
        "Let's start doing some classification on the Heart Disease dataset from Kaggle at https://www.kaggle.com/ronitf/heart-disease-uci. The dataset has 303 patients and 13 features for each patient. \n",
        "\n",
        "You can find the dataset at the link above, or directly load it like shown below: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BNccumSb8iY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNl7rqzW6EQ8",
        "colab_type": "text"
      },
      "source": [
        "Let's make the data nicer. Since the column headers are a bit heard to read, we can override them to provide more easy to understand labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uwN838fePM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Give a better name to each column\n",
        "data.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',\n",
        "       'exercise_induced_angina', 'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'target']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BzbbuFFm3-L",
        "colab_type": "text"
      },
      "source": [
        "Let's check how balanced our dataset is. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OONYrA4bm3zN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO77DQcZKFS_",
        "colab_type": "text"
      },
      "source": [
        "It's almost balanced, but not entirely, so during evaluation, let's make sure to check F1 values and AUC instead of just accuracy. \n",
        "\n",
        "We also need to make sure that the system recognises our data as being categorical appropriately. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_79UlghVU0ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzs1jAItU0lX",
        "colab_type": "text"
      },
      "source": [
        "This looks good. Everything is an integer, except for the continuous variables. In the case where categories are stored as strings, you need to correct this with: \n",
        "\n",
        "data['Class'] = pd.Categorical(data['Class'])\n",
        "\n",
        "data['Class'] = data['Class'].cat.codes\n",
        "\n",
        "In our case all is well. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XinuntO4z3Q0",
        "colab_type": "text"
      },
      "source": [
        "Looking at the header, the prediction task is now clear: we need to predict if target = 1, meaning heart disease is present. \n",
        "\n",
        "To properly evaluate models, we **create a test and training set** with 30% split using train_test_split. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX4WMdCUh5z6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-Q1FrzvVV7Q",
        "colab_type": "text"
      },
      "source": [
        "Most classifiers will work best if the data is normalized. We can easily do this by using the MinMaxScaler fuction. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_E3AMIc9tzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S9UfiOQT4ml",
        "colab_type": "text"
      },
      "source": [
        "### Model 1: Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYOgQbN1UIgs",
        "colab_type": "text"
      },
      "source": [
        "Let's import the sklearn library for CART decision trees, and define a variable 'model' to hold our CART tree model. We can specify some pruning/early stopping parameters so that the size of the tree can be adjusted. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xrgj22A9T4bt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjfBdBecT4QN",
        "colab_type": "text"
      },
      "source": [
        "Let's train the model by passing it our training data X_train and labels y_train. Note that decision trees don't need normalized data, in this case it may be easier to understand with the original data (so we are using X_train, not X_train_norm as we do in the other models): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlmTUjBFT3pN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omH5Mwu_UMSd",
        "colab_type": "text"
      },
      "source": [
        "Notice how the options above let you tweek the classifier (e.g. criterion Gini coefficient). This model trained really fast! But how good is it?\n",
        "\n",
        "I've emphasized the importance of evaluating models using multiple metrics in class. Let's **define a function called\n",
        "evaluate_on_training_set, which will print out all of out metrics**. This will save us some time throughout the lab. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwqSjJrgXYvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6oR4zwwYWTg",
        "colab_type": "text"
      },
      "source": [
        "Using this new function we can now make predictions for our newly trained model 'model' and get the evaluation metrics: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAZ4xBNMUOdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caPHKHy7wq54",
        "colab_type": "text"
      },
      "source": [
        "Finally, what does the actual tree look like? You can experiment yourself how the tree (and the accuracy) changes when we set different pruning parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owDNZ2-PwJvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ej4HeSRpAbyV"
      },
      "source": [
        "### Model 2: K Nearest Neighbor Classifier\n",
        "\n",
        "Let's explore the kNN algorithm. As you've seen in class. It's a very simple algorithm, however, it needs a lot of time to compute all distances. \n",
        "\n",
        "Similar to before, we first load the library and define the model with parameters (here only the parameter k is needed). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n50Cahe4Abyu",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cfKnOPbdAbzB"
      },
      "source": [
        "We can then proceed to train the model using the fit command and evaluate it using the function we defined before: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5wZLBhLzAbzH",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg9vW0qE5XUl",
        "colab_type": "text"
      },
      "source": [
        "It's extremely important to set a good value for k, so you will need to test this. E.g. try running the above code with k = 5. See any change? Your F1 scores should go up! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4zwXi4ZhrfI",
        "colab_type": "text"
      },
      "source": [
        "### Model 3: Logistic Regression\n",
        "\n",
        "Let's move on to a very powerful, yet fast model: Logistic Regression. Again, we start by loading the library and defining the model together with its parameters. \n",
        "\n",
        "In this case, multi_class auto will detect the number of classes automatically, C is our regularisation parameter, and solver is the optimization algorithm used to fit the model: \n",
        "* For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones.\n",
        "* For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss; ‘liblinear’ is limited to one-versus-rest schemes.\n",
        "* ‘newton-cg’, ‘lbfgs’, ‘sag’ and ‘saga’ handle L2 or no penalty\n",
        "* ‘liblinear’ and ‘saga’ also handle L1 penalty\n",
        "* ‘saga’ also supports ‘elasticnet’ penalty\n",
        "* ‘liblinear’ does not handle no penalty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLIt5YndT50m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnxXpOEAT5Xv",
        "colab_type": "text"
      },
      "source": [
        "We can the train and evaluate the model like usual: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QBgzuzXT5RQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccCDO_059GQt",
        "colab_type": "text"
      },
      "source": [
        "This is a fast model again, with rather good output. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PDH79leQBhhG"
      },
      "source": [
        "### Model 4: Gaussian Naive Bayes\n",
        "\n",
        "Let's create a Naive Bayes model. There are a [number of variants on this model to choose](https://scikit-learn.org/stable/modules/naive_bayes.html) from in Scikit learn, let's pick the Gaussian variant. Same thing as always, we load the library and define the model: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OGd4wUJlBhhS",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sJG3BrjMBhhd"
      },
      "source": [
        "We can proceed to train (fit) the model and evaluate: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K7gc7fHHBhhi",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fFvlmdg-WKN",
        "colab_type": "text"
      },
      "source": [
        "Not a bad result! Gaussian models often perform well with smaller datasets as they fit the limited data to a distribution. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y29Xe17lV2NX",
        "colab_type": "text"
      },
      "source": [
        "### Model 5: SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pQZCVX9SiL7",
        "colab_type": "text"
      },
      "source": [
        "Support Vector Machines offer a very powerful way to transform our data by implementing a 'kernel trick'. Hence, when we define our model, we need to specify our kernel. The results will be highly dependant on the kernel value and the regularisation parameter C. \n",
        "\n",
        "The kernel can be: ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’. If none is given, ‘rbf’ will be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6HvqJQAV5Fi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apT-Sk1hSmPp",
        "colab_type": "text"
      },
      "source": [
        "As usual, we use the exact same commands to train and evaluate the model: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngMA8wrOSruL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssew1Y4f_O-x",
        "colab_type": "text"
      },
      "source": [
        "##### optimizing SVM using GridSearch \n",
        "Were you expecting better results from such a powerful model? Let's tweek the parameters to improve our accuracy...\n",
        "\n",
        "We will be using Grid Search to tune parameters and try a range: \n",
        "- with kernel rbf, try varying gamma (which is a coefficient in the rbf kernel) to be 1e-3 or 1e-4; and vary C to be 1, 10, or 100.\n",
        "- with linear kernel (there is no gamma here), try varying C to be 1, 10, and 100.\n",
        "\n",
        "These values are stored as tuned_parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCvou1fXTa7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjX_fJEgTa21",
        "colab_type": "text"
      },
      "source": [
        "Using the above parameters, we can run a GridSearch algorithm to optimize the AUC score. The verbose output will output the process; n_jobs will parallize the computation over 4 processors; 2-fold CV is used during the search (cv = 2). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPzkCidqTaxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7wpv63TisBw",
        "colab_type": "text"
      },
      "source": [
        "Once this is trained we can again look at what the best parameters are and how well the model performs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48-O9Iz3irgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "shn7lQyfAwZI"
      },
      "source": [
        "### Model 5: AdaBoostClassifier (Ensemble method)\n",
        "\n",
        "Ensemble models such as AdaBoost build multiple models and leverage the output to get a better result (most of the time). Sklearn tells us that: \"AdaBoost is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\"\n",
        "\n",
        "Let's define our model and set some parameters, learning rate can vary from 0 to 1; and n_estimators is the number of times a model is built. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_UweHzVKAwZi",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oIQpSZURAwZx"
      },
      "source": [
        "How does it perform? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b62R66_EAwaE",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xbMP0yiBSXH",
        "colab_type": "text"
      },
      "source": [
        "### Model 6: RandomForestClassifier\n",
        "\n",
        "Another ensemble classifier is Random Forest, which takes a bagging approach. We define the model: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kzrmpvqnBNWZ",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6OzZd-UoBNWW"
      },
      "source": [
        "You can experiment with the parameters above (n_estimators is how many trees there are in the model), until you get good results. More parameters about the trees can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mydCJ77ZBNWi",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gVcGUR7QBNWq"
      },
      "source": [
        "This is an excellent model and trained quite fast. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENhvgZA-cUyB",
        "colab_type": "text"
      },
      "source": [
        "# Let's try on a new dataset..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN8PoYrvlLUT",
        "colab_type": "text"
      },
      "source": [
        "Find the best performing model to predict student's academic performance and the following dataset: https://www.kaggle.com/aljarah/xAPI-Edu-Data. The goal is to estimate the grade level of the student: \n",
        "\n",
        "The students are grouped into three numerical intervals based on their total grade/mark:\n",
        "* Low-Level: interval includes values from 0 to 69,\n",
        "* Middle-Level: interval includes values from 70 to 89,\n",
        "* High-Level: interval includes values from 90-100.\n",
        "\n",
        "Some steps for you to follow: \n",
        "\n",
        "1. Load the csv file as dataframe directly from\n",
        "\n",
        " https://dorienherremans.com/drop/CDS/classification/xAPI-Edu-Data.csv\n",
        "2. Check if columns are recognised as categorical, is not, change them to categorical with the command: \n",
        "\n",
        "for key in ['gender','NationalITy','PlaceofBirth','StageID','GradeID','SectionID','Topic','Semester',\n",
        "'Relation',\n",
        "\n",
        "'ParentAnsweringSurvey','ParentschoolSatisfaction',\n",
        "'StudentAbsenceDays','Class']:\n",
        "\n",
        "    data[key] = pd.Categorical(data[key])\n",
        "    data[key] = data[key].cat.codes\n",
        "\n",
        "3. Preprocess the data to create a normalized training set with 30% split.\n",
        "4. Train and evaluate multiple models to find the best classifier. \n",
        "\n",
        "IMPORTANT: Evaluate **only** in terms of classification_report and confusion matrix. (No need for AUC and ROC here, as we are doing multiclass classification, the target label would need to be binarized first, which we are not doing.)\n",
        "\n",
        "We are trying to reach the **best recall performance** for predicting high grade students (class 0), in other words: number of correctly predicted high grade students over the total number of high grade students. \n",
        "\n",
        "**Fill in your best performing recall score for class 0 in our high score list: \n",
        "https://forms.gle/wquRxxjVBfgnVtht8**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BjcZHuoYbWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCDWq6MXxLPw",
        "colab_type": "text"
      },
      "source": [
        "Helpful code to categorise the attributes (and save you typing time): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgE5EN9AxLW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for key in ['gender','NationalITy','PlaceofBirth','StageID','GradeID','SectionID','Topic','Semester','Relation','ParentAnsweringSurvey','ParentschoolSatisfaction','StudentAbsenceDays','Class']:\n",
        "\n",
        "    data[key] = pd.Categorical(data[key])\n",
        "    data[key] = data[key].cat.codes\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}