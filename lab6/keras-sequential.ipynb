{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install keras\n",
    "# pip install tensorflow (backend)\n",
    "# pip install pydot\n",
    "# pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "A local file was found, but it seems to be incomplete or outdated because the auto file hash does not match the original value of 599dadb1135973df5b59232a0e9a887c so we will re-download the data.\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 4s 0us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Build model...\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 80, 128)           2560000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10240)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1310848   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 3,870,977\n",
      "Trainable params: 3,870,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pengfei/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/15\n",
      "22500/22500 [==============================] - 22s 975us/step - loss: 0.4356 - accuracy: 0.7872 - val_loss: 0.3650 - val_accuracy: 0.8304\n",
      "Epoch 2/15\n",
      "22500/22500 [==============================] - 21s 948us/step - loss: 0.0468 - accuracy: 0.9854 - val_loss: 0.7281 - val_accuracy: 0.8016\n",
      "Epoch 3/15\n",
      "22500/22500 [==============================] - 21s 950us/step - loss: 0.0048 - accuracy: 0.9988 - val_loss: 0.7923 - val_accuracy: 0.8088\n",
      "Epoch 4/15\n",
      "22500/22500 [==============================] - 22s 958us/step - loss: 5.4395e-04 - accuracy: 0.9999 - val_loss: 0.8466 - val_accuracy: 0.8128\n",
      "Epoch 5/15\n",
      "22500/22500 [==============================] - 22s 997us/step - loss: 5.6661e-05 - accuracy: 1.0000 - val_loss: 0.8737 - val_accuracy: 0.8104\n",
      "Epoch 6/15\n",
      "22500/22500 [==============================] - 23s 1ms/step - loss: 2.7765e-05 - accuracy: 1.0000 - val_loss: 0.8913 - val_accuracy: 0.8112\n",
      "Epoch 7/15\n",
      "22500/22500 [==============================] - 24s 1ms/step - loss: 1.8073e-05 - accuracy: 1.0000 - val_loss: 0.9090 - val_accuracy: 0.8120\n",
      "Epoch 8/15\n",
      "22500/22500 [==============================] - 23s 1ms/step - loss: 1.2153e-05 - accuracy: 1.0000 - val_loss: 0.9257 - val_accuracy: 0.8124\n",
      "Epoch 9/15\n",
      "22500/22500 [==============================] - 23s 1ms/step - loss: 8.2698e-06 - accuracy: 1.0000 - val_loss: 0.9444 - val_accuracy: 0.8128\n",
      "Epoch 10/15\n",
      "22500/22500 [==============================] - 23s 1ms/step - loss: 5.6718e-06 - accuracy: 1.0000 - val_loss: 0.9618 - val_accuracy: 0.8132\n",
      "Epoch 11/15\n",
      "22500/22500 [==============================] - 23s 1ms/step - loss: 3.9079e-06 - accuracy: 1.0000 - val_loss: 0.9796 - val_accuracy: 0.8128\n",
      "25000/25000 [==============================] - 1s 43us/step\n",
      "Test score: 1.042145834121704\n",
      "Test accuracy: 0.8118799924850464\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras.callbacks import EarlyStopping\n",
    "max_features = 20000\n",
    "maxlen = 80 # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) # Replace this and create your own data loader and pre-processing for the given data on e-dimension\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)\n",
    "model.add(Embedding(max_features, 128,input_shape=(maxlen,),trainable=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=10)\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "batch_size=batch_size,\n",
    "epochs=15,\n",
    "validation_split=0.1, callbacks=[es])\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
