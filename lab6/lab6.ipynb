{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "max_features = 20000\n",
    "maxlen = 80 # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = pd.read_csv('imdb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean(html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    all_text = ''.join(soup.findAll(text=True))\n",
    "    s = \"123456789\" + string.punctuation\n",
    "    result = ''.join([i if i not in s else ' ' for i in all_text])\n",
    "    result=result.replace(\"  \",\" \")\n",
    "    return result\n",
    "imdb.review = imdb.review.apply(clean)\n",
    "imdb.sentiment = pd.Categorical(imdb.sentiment)\n",
    "imdb.sentiment = imdb.sentiment.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(imdb.review, imdb.sentiment, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000) #Define a Keras tokenizer; \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer.fit_on_texts(X_train) # Fit the tokenizer on the text\n",
    "X_train = tokenizer.texts_to_sequences(X_train) # Convert the text to sequences\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 80, 100)           2000000   \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               1024128   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 3,024,257\n",
      "Trainable params: 3,024,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pengfei/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36000 samples, validate on 4000 samples\n",
      "Epoch 1/5\n",
      "36000/36000 [==============================] - 31s 865us/step - loss: 0.4163 - accuracy: 0.8008 - val_loss: 0.3702 - val_accuracy: 0.8357\n",
      "Epoch 2/5\n",
      "36000/36000 [==============================] - 30s 822us/step - loss: 0.1717 - accuracy: 0.9340 - val_loss: 0.5225 - val_accuracy: 0.8110\n",
      "Epoch 3/5\n",
      "36000/36000 [==============================] - 30s 827us/step - loss: 0.0309 - accuracy: 0.9886 - val_loss: 0.8918 - val_accuracy: 0.8020\n",
      "Epoch 4/5\n",
      "36000/36000 [==============================] - 30s 839us/step - loss: 0.0219 - accuracy: 0.9920 - val_loss: 0.9833 - val_accuracy: 0.7980\n",
      "Epoch 5/5\n",
      "36000/36000 [==============================] - 31s 848us/step - loss: 0.0182 - accuracy: 0.9936 - val_loss: 1.1879 - val_accuracy: 0.8020\n",
      "10000/10000 [==============================] - 0s 44us/step\n"
     ]
    }
   ],
   "source": [
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "model = Sequential()\n",
    "# keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)\n",
    "model.add(Embedding(max_features, 100, input_shape=(maxlen,),trainable=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=10)\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=5, validation_split=0.1, callbacks=[es])\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8026000261306763"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
